---
title: "Session 2"
subtitle: "Model Evaluation and Optimization"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(keras)
library(tidyverse)
library(cowplot)

knitr::opts_chunk$set(echo = FALSE)
```

## Overfitting

```{r MC_Overfitting}

quiz(caption = "Overfitting",
  question("Which methods to alleviate overplotting did we discuss?",
           answer("Increase sample size"),
           answer("Decrease Capacity)"),
           answer("Add weight regularization"),
           answer("Add dropout", correct = TRUE),
           allow_retry = T,
           correct = "Great!"
  ),
  question("We can observe overfitting in our history as:",
           answer("The validation loss does not increase"),
           answer("The training loss decreases, while the validation loss increases"),
           answer("The validation loss is ", correct = TRUE),
           answer("Categorical response variable"),
           allow_retry = T,
           correct = "Great!"
  ),
  question("I made the distinction between data and algorithim-based methods for dealing with overfitting. What did I mean by data-based?",
           answer("We should standardize out values so that they are not is wildely different ranges", message = "This is correct, and will help when training your model, but it's not quite what I meant"),
           answer("That we should have lots of instances.", correct = TRUE),
           answer("That we should have lots of features."),
           answer("That we should pay close attention to feature engineering", message = "While you should certainly pay attention to the features you're using, deep learning can relieve the need to extensive feature engineering"),
           allow_retry = T,
           correct = "Great!"
  ),
  question("A models capacity refers to:",
           answer("The hyperparameters, i.e. the number of hidden layers and the number of nodes in each hidden layer"),
           answer("The parameters, i.e. the weights and bias matrices.", correct = TRUE),
           answer("The validation accuracy of the model."),
           answer("The ability for the model to accurately classify an instance of the test set."),
           allow_retry = T,
           correct = "Great!"
  ),
  question("At the start of training, will the validation loss be small or greater than the training loss?",
           answer("Smaller"),
           answer("Larger", correct = TRUE),
           allow_retry = T,
           correct = "Great! Can you explain why?"
  ),
  question("L1 weight regularization refers to:",
           answer("A", correct = TRUE),
           answer("B", correct = TRUE),
           answer("C", correct = TRUE),
           answer("D"),
           allow_retry = T,
           correct = "Great!"
  ),
  question("L2 Weight regularization refers to:",
           answer("Many variables", correct = TRUE),
           answer("A plot with too many endoding aesthetics"),
           answer("Something to do with Captain Picard"),
           answer("A high-dimensional feature space", correct = TRUE),
           allow_retry = T,
           correct = "Great!"
  ),
  question("True or False: Larger models are always prefered since we can account for more subltilities in the training data",
           answer("A", correct = TRUE),
           answer("B"),
           answer("C"),
           answer("D", correct = TRUE),
           allow_retry = T,
           correct = "Great!"
  ),
  question("The more features we have in our test set:",
           answer("The more hidden layers we should include"),
           answer("Each hidden layer needs to be larger", correct = TRUE),
           answer("We have a better "),
           answer("Too much emphasis on data frames over lists"),
           allow_retry = T,
           correct = "Great!"
  ),
  question("Large, complex models lead to overfitting because:",
           answer("This statement is false, actually, they perform better than smaller models"),
           answer("They begin to 'memorize' paths unique to the test set", message = "Exactly, they begin to specialize on the training set.", correct = TRUE),
           answer("They generalize well to many difference instances",  message = "This is not really the case, they are not generalizing, they are actually getting more specialized -- to the training set."),
           answer("D"),
           allow_retry = T,
           correct = "Great!"
  ),
  question("L1 weight regularization refers to:",
           answer("Absolute values", correct = TRUE),
           answer("Squared values"),
           answer(""),
           answer("Too much emphasis on data frames over lists"),
           allow_retry = T,
           correct = "Great!"
  )
  
)

```

## Evaluation

```{r MC_Evaluation}

quiz(caption = "Model Evaluation",
  question("Given the following figure, what is the...",
           answer("Increase sample size"),
           answer("Decrease Capacity)"),
           answer("Add weight regularization"),
           answer("Add dropout", correct = TRUE),
           allow_retry = T,
           correct = "Great!"
  ),
  question("We can observe overfitting in our history as:",
           answer("The validation loss does not increase"),
           answer("The training loss decreases, while the validation loss increases"),
           answer("The validation loss is ", correct = TRUE),
           answer("Categorical response variable"),
           allow_retry = T,
           correct = "Great!"
  ),
  question("At the start of training, will the validation loss be small or greater than the training loss?",
           answer("Smaller"),
           answer("Larger", correct = TRUE),
           allow_retry = T,
           correct = "Great! Can you explain why?"
  ),
  question("L1 weight regularization refers to:",
           answer("A", correct = TRUE),
           answer("B", correct = TRUE),
           answer("C", correct = TRUE),
           answer("D"),
           allow_retry = T,
           correct = "Great!"
  ),
  question("L2 Weight regularization refers to:",
           answer("Many variables", correct = TRUE),
           answer("A plot with too many endoding aesthetics"),
           answer("Something to do with Captain Picard"),
           answer("A high-dimensional feature space", correct = TRUE),
           allow_retry = T,
           correct = "Great!"
  ),
  question("True or False: Larger models are always prefered since we can account for more subltilities in the training data",
           answer("A", correct = TRUE),
           answer("B"),
           answer("C"),
           answer("D", correct = TRUE),
           allow_retry = T,
           correct = "Great!"
  ),
  question("The more features we have in our test set:",
           answer("The more hidden layers we should include"),
           answer("Each hidden layer needs to be larger", correct = TRUE),
           answer("We have a better "),
           answer("Too much emphasis on data frames over lists"),
           allow_retry = T,
           correct = "Great!"
  )
  
)

```


## Your Turn



```{r MC_Evaluation}

quiz(caption = "Model Evaluation",
  question("Given the following figure, what is the...",
           answer("Increase sample size"),
           answer("Decrease Capacity)"),
           answer("Add weight regularization"),
           answer("Add dropout", correct = TRUE),
           allow_retry = T,
           correct = "Great!"
  ),
  question("We can observe overfitting in our history as:",
           answer("The validation loss does not increase"),
           answer("The training loss decreases, while the validation loss increases"),
           answer("The validation loss is ", correct = TRUE),
           answer("Categorical response variable"),
           allow_retry = T,
           correct = "Great!"
  ),
  question("At the start of training, will the validation loss be small or greater than the training loss?",
           answer("Smaller"),
           answer("Larger", correct = TRUE),
           allow_retry = T,
           correct = "Great! Can you explain why?"
  ),
  question("L1 weight regularization refers to:",
           answer("A", correct = TRUE),
           answer("B", correct = TRUE),
           answer("C", correct = TRUE),
           answer("D"),
           allow_retry = T,
           correct = "Great!"
  ),
  question("L2 Weight regularization refers to:",
           answer("Many variables", correct = TRUE),
           answer("A plot with too many endoding aesthetics"),
           answer("Something to do with Captain Picard"),
           answer("A high-dimensional feature space", correct = TRUE),
           allow_retry = T,
           correct = "Great!"
  ),
  question("True or False: Larger models are always prefered since we can account for more subltilities in the training data",
           answer("A", correct = TRUE),
           answer("B"),
           answer("C"),
           answer("D", correct = TRUE),
           allow_retry = T,
           correct = "Great!"
  ),
  question("The more features we have in our test set:",
           answer("The more hidden layers we should include"),
           answer("Each hidden layer needs to be larger", correct = TRUE),
           answer("We have a better "),
           answer("Too much emphasis on data frames over lists"),
           allow_retry = T,
           correct = "Great!"
  )
  
)

```
