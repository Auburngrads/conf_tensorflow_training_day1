---
title: "Model Evaluation and Optimization MNIST"
output: html_document
runtime: shiny_prerendered
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Initialize package  
# install.packages(keras)
library(keras)
```

# Install tensorflow 

It's only necessary to run this once. 

```{r install, eval = F}
# for GPU
# install_keras(tensorflow = "gpu")

# or CPU:
# install_keras() # for cpu
```

## Part 1: Data Preparation

## Obtain data

```{r data}
c(c(train_images, train_labels), c(test_images, test_labels)) %<-% dataset_mnist()
```

## View image

```{r plot}
digit <- train_images[1,,]
plot(as.raster(digit, max = 255))
```

## Normalize data:

```{r strImagesPre}
str(train_images)
str(test_images)
```

```{r normImages}
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
str(train_images)
```


```{r normImages}
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
str(test_images)
```

## Prepare labels:

```{r strLabelsPre}
train_labels_int <- train_labels
str(train_labels)

test_labels_int <- test_labels
str(test_labels)
```

```{r prepLabels}
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
```

```{r strLabelsPost}
str(train_labels)
str(test_labels)
```

# Part 2: Define Network

## Define the network

```{r architecture}
network <- keras_model_sequential() %>%
  layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%
  layer_dense(units = 10, activation = "softmax")
```

## View a summary of the network

```{r summary}
summary(network)
```

## Compile

```{r compile}
network %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)
```

## Train the model

```{r train}
history <- network %>% 
  fit(train_images, train_labels, epochs = 5, batch_size = 128)
```

## Plot progress

```{r plotTraining}
plot(history)
```

# Part 3: Check output

## Metrics

```{r metrics}
metrics <- network %>% evaluate(test_images, test_labels)
metrics

metrics$acc

# Error rate: incorrect calling
1 - metrics$acc

```

## Predictions

```{r predictions}
network %>% predict_classes(test_images[1:10,])
```

```{r allPredictions}
predictions <- network %>% predict_classes(test_images)

actual <- test_labels_int
totalmisses <- sum(predictions != actual)
```

# Confusion Matrix

```{r confusion, echo = F}
suppressPackageStartupMessages(library(tidyverse))

data.frame(target = test_labels_int,
                      prediction = network %>%
                        predict_classes(test_images)) %>% 
  filter(target != prediction) %>% 
  group_by(target, prediction) %>%
  count() %>%
  ungroup() %>%
  mutate(perc = n/nrow(.)*100) %>% 
  filter(n > 1) %>% 
  ggplot(aes(target, prediction, size = n)) +
  geom_point(shape = 15, col = "#9F92C6") +
  scale_x_continuous("Actual Target", breaks = 0:9) +
  scale_y_continuous("Prediction", breaks = 0:9) +
  scale_size_area(breaks = c(2,5,10,15), max_size = 5) +
  coord_fixed() +
  ggtitle(paste(totalmisses, "mismatches")) +
  theme_classic() +
  theme(rect = element_blank(),
        axis.line = element_blank(),
        axis.text = element_text(colour = "black"))

```

## Variants:


---
title: "Tuning hyperparameters with the IMDB data set"
author: "Rick Scavetta" 
---

```{r setup, context="setup", include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Initialize package
# install.packages(keras)
library(keras)
library(tidyverse) # purrr for reiterations, dplyr for data handling
plot_bkg <- "grey70"

# Install tensorflow - It's only necessary to run this once. 
# for GPU
# install_keras(tensorflow = "gpu")

# or CPU:
# install_keras() # for cpu
```

We already saw the Reuters case study as described in the book. Let's take deeper look at some of the variants described in the exercises.

# Part 1: Data Preparation

## Obtain and prepare data

We already examined the data in the previous script. Here, we'll just prepare the data as before, for use with binary crossentropy. If you're unfamiliar with what's happening here, please refer to the main script. 

```{r data, warning = FALSE, echo = FALSE, context = "data", cache = TRUE}
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% dataset_imdb(num_words = 10000)

# Prepare the data using one-hot encoding
vectorize_sequences <- function(sequences, dimension = 10000) {
  # Create a matrix of 0s
  results <- matrix(0, nrow = length(sequences), ncol = dimension)

  # Populate the matrix with 1s
  for (i in 1:length(sequences))
    results[i, sequences[[i]]] <- 1
  results
}

train_data_vec <- vectorize_sequences(train_data)
test_data_vec <- vectorize_sequences(test_data)

rm(train_data)
rm(test_data)

# Prepare the labels
train_labels <- as.numeric(train_labels)
test_labels <- as.numeric(test_labels)

# Prepare the validation set
index <- 1:10000

val_data_vec <- train_data_vec[index,]
train_data_vec <- train_data_vec[-index,]

val_labels <- train_labels[index]
train_labels <- train_labels[-index]
```

# Variant 1: Larger or smaller layers

In the original version, we used $2^4 = 16$ neurons in each of our two hidden layers. Here, we'll try the whole range from $2^2 = 4$ to $2^{6} = 64$ neurons in each hidden layer.

To do this, we'll define a function, `define_model` that allows us to define and compile our model. It takes one argument: 

- `powerto = 4` - integer, the number of neurons as defined by two to the power of this value.

```{r define_model}
# Generalise the model definitions
# Defaults to parameters we used in original case study.
define_model <- function(powerto = 4) {
  
  # cat("Defining model with ", 2^powerto, " neurons per hidden layer \n")
  
  # Define the model, using powerto arg for neuron number.
  network <- keras_model_sequential() %>% 
    layer_dense(units = 2^powerto, activation = "relu", input_shape = c(10000)) %>% 
    layer_dense(units = 2^powerto, activation = "relu") %>% 
    layer_dense(units = 1, activation = "sigmoid")
  
  # Compile as before
  network %>% compile(
    optimizer = "rmsprop",
    loss = "binary_crossentropy",
    metrics = c("accuracy")
  )
}

```

In addition to that, I'll define another function for training the model, `run_model`. It takes one argument:

- `epochs = 20` - integer, the numer of training epochs.

I won't actually change the number of epochs here. it's provided for your convenience. I've set `verbose = FALSE` to avoid lots of print out.

```{r run_model}
run_model <- function(network, epochs = 20) {
  
  # cat("Training model ... \n")

  # Train the model and return the history (or just the network)
    network %>% fit(
      train_data_vec,
      train_labels,
      epochs = epochs,
      batch_size = 512,
      validation_data = list(val_data_vec, val_labels),
      verbose = FALSE
    )
}
```

The data is built into the function definitions, so calling my functions as below will provide the results we've seen previously.

```{r ori_run}
define_model() %>% 
  run_model() -> history_original 

# Plot history using default:
history_original %>%
  plot()

# Prep data frame for use later on:
history_original %>% 
  data.frame() %>% 
  mutate(nlayers = "2",
         powerto = "16") -> history_original
```

I'm using `purrr::map()` to calculate all the models reiteratively. Since I want to plot all the values together, I convert the history to a data frame. At the end we'll have one large data frame with a `powerto` column that tells use how many neurons that model used.

```{r var1_run, context = "data", cache = TRUE}
# Define number of neurons
powerto_input <- c(2:3,5:6)

powerto_input %>% 
  map(define_model) %>% 
  map(run_model) %>% 
  map_df(data.frame, .id = "powerto") %>% 
  mutate(powerto = as.character(factor(powerto, labels = 2^powerto_input)),
         nlayers = "2") -> history_powerto
```

From this, we can look at how the validation set accuracy and loss are affected. The line marked with red dots are the values we used in the original definition, above.

```{r var1_plot, echo = F, message = FALSE}
# merge with original and plot:
history_powerto %>% 
  full_join(history_original) %>%
  arrange(as.numeric(powerto)) %>% 
  ggplot(aes(epoch, value, col = as_factor(powerto), alpha = as_factor(powerto))) +
  geom_line(alpha = 0.6) +
  geom_point(col = "red") +
  scale_alpha_manual("Number of neurons", values = c(rep(0,2),1,rep(0,2))) +
  scale_color_brewer("Number of neurons", palette = "Blues") +
  facet_grid(metric ~ data, scales = "free_y") +
  labs(title = "2 hidden layers, changing number of neurons") +
  theme_classic() +
  theme(panel.background = element_rect(fill = plot_bkg),
        legend.key = element_rect(fill = plot_bkg))
```

Compared to the differently sized layers, our model reaches a fairly high and consistent accuracy. The larger models do not improve much on our original result.

We can also count the number of parameters in each model:

```{r nparams1, echo = FALSE}
powerto_input2 <- 2:10

powerto_input2 %>% 
  map(define_model) -> justModels_powerto

map(justModels_powerto, count_params) %>% 
  map_df(data.frame, .id = "neurons") %>% 
  mutate(neurons = 2^powerto_input2,
         nlayers = 2) -> justModels_powerto 

ggplot(justModels_powerto, aes(log2(neurons), .x..i..)) +
  geom_line() +
  geom_point(shape = 16, alpha = 0.6, size = 5) +
  scale_x_continuous("Number of neurons", breaks = powerto_input2, labels = 2^powerto_input2) +
  labs(y = "Number of parameters") +
  theme_classic() +
  theme(panel.background = element_rect(fill = plot_bkg),
        legend.key = element_rect(fill = plot_bkg))
```

# Variant 2: Changing the number of layers layers

So now we have an idea that 16 neurons is appropriate. Do you think it would help to change the number of layers? I'm going to expand on the `build_model()` function I established earlier to allow us to define how many hidden layers.

- `nlayers = 2` integer, the number of hidden layers. Minimum is 1.

To make my syntax with `purrr::map()` easier, I'm going to put it in the first position.

```{r define_model_revisited, context = "data", cache = TRUE}
define_model <- function(nlayers = 2, powerto = 4) {

  # cat("Defining model with ", 2^powerto, " neurons per ", nlayers," hidden layer(s) \n")
  
  # Establish model with single hidden, input, layer
   network <- keras_model_sequential() %>% 
    layer_dense(units = 2^powerto, activation = "relu", input_shape = c(10000))

  # Add nlayers-1 number of additional layers
  if (nlayers > 1) {
  map(2:nlayers, ~ network %>%
        layer_dense(units = 2^powerto, activation = "relu") 
  )
  }
  
  # Add final layer  
  network %>% 
    layer_dense(units = 1, activation = "sigmoid")
  
  network %>% compile(
    optimizer = "rmsprop",
    loss = "binary_crossentropy",
    metrics = c("accuracy")
  )
  
}

```

Once again, I'll use `purrr::map()` to calculate all the models reiteratively. This time I'll have one large data frame and the `nlayers` column will correspond to how many layers that model contained.

```{r var2_run, context = "data", cache = TRUE}
# Define number of neurons
nlayers_input <- c(1,3:4)

nlayers_input %>% 
  map(define_model) %>%
  map(run_model) %>% 
  map_df(data.frame, .id = "nlayers") %>% 
  mutate(nlayers = as.character(factor(nlayers, labels = nlayers_input)),
         powerto = "16") -> history_nlayers

```

```{r var2_plot, context = "data", cache = TRUE, echo = FALSE, message = FALSE}
history_nlayers %>%
  full_join(history_original) %>%
  ggplot(aes(epoch, value, col = nlayers, alpha = nlayers)) +
  geom_line(alpha = 0.6) +
  geom_point(col = "red") +
  scale_alpha_manual("Number of\nhidden layers", values = c(0,1,rep(0,6))) +
  scale_color_brewer("Number of\nhidden layers", palette = "Blues") +
  facet_grid(metric ~ data, scales = "free_y") +
  labs(title = "16 neurons per layer, changing number of hidden layers") +
  theme_classic() +
  theme(panel.background = element_rect(fill = plot_bkg),
        legend.key = element_rect(fill = plot_bkg))
```

Our accuracy and loss are not improved upon by increasing the number of layers. Actually, more layers seems to preform worse!  

Let's take a look at the number of parameters here:

```{r nparams2, echo = FALSE, message = FALSE, cache = FALSE}
nlayers_input2 <- 1:4

nlayers_input2 %>% 
  map(define_model) -> justModels_nlayer

map(justModels_nlayer, count_params) %>%
   map_df(data.frame, .id = "nlayers") %>% 
   mutate(neurons = 16,
          nlayers = nlayers_input2) -> justModels_nlayer

justModels_nlayer[3:1] %>%
  bind_rows(justModels_powerto) %>% 
  filter(!duplicated(.)) %>% 
  mutate(nlayers = as.factor(nlayers)) -> parameters_total

g <- ggplot(parameters_total, aes(log2(neurons), .x..i.., col = nlayers)) +
  geom_line() +
  geom_point(shape = 16, alpha = 0.6, size = 5) +
  scale_x_continuous("Number of neurons", breaks = powerto_input2, labels = 2^powerto_input2) +
  labs(y = "Number of parameters", col = "Number of\nhidden layers") +
  theme_classic() +
  theme(panel.background = element_rect(fill = plot_bkg),
        legend.key = element_rect(fill = plot_bkg))
  
# Zoom in on region of interest
g + coord_cartesian(xlim = c(2.5,5.5), ylim = c(80000, 325000))
  
```

# Variant 3: Combination of varying number of neurons and hidden layers

Since we're here, let's look at changing both at the same time. We'll take a look at all possible combinations between `r min(nlayers_input)` and `r max(nlayers_input)` hidden layers and $2^{`r powerto_input[1]`}$ and $2^{`r powerto_input[length(powerto_input)]`}$ neurons per layer. Use the controls to view the results.

```{r var3_run, context = "data", cache = TRUE, echo = FALSE}
# Make a data frame of all possible combinations,
# but remove those already calculated:
  expand.grid(nlayers_input,
              powerto_input) %>% 
  rename(nlayers_input = Var1,
         powerto_input = Var2) -> DF 

# Calculate all possible combinations:
map2(DF$nlayers_input, DF$powerto_input, ~ define_model(.x, .y)) %>% 
  map(run_model) %>% 
  map_df(data.frame, .id = "ID") %>% 
  mutate(ID = factor(ID, labels = paste(DF$nlayers_input, 2^DF$powerto_input, sep = "_"))) %>% 
  separate(ID, c("nlayers", "powerto")) -> history_rest


```

```{r data_merge, echo = FALSE, message = FALSE}
# Join with the rest of the data
# Remove duplicate entries before merging:
history_rest %>% 
  full_join(history_original) %>% 
  full_join(history_nlayers) %>% 
  full_join(history_powerto) %>%
  mutate(nlayers = as.numeric(nlayers),
         powerto = as.numeric(powerto))  %>% 
  arrange(powerto) -> history #%>% 
```

```{r var3_plot_shiny_app, echo = FALSE}
# Create a shinyUI for exploring the data:
inputPanel(
  sliderInput("layers", h4("Number of layers:"),
              min = min(history$nlayers), max = max(history$nlayers),
              value = 2, step = 1,
              animate = animationOptions(interval = 1000, loop = TRUE)),
  
  radioButtons("metric", h4("Metric"),
               choices = list("Loss" = "loss", "Accuracy" = "acc"),
               selected = "acc"),
  
    radioButtons("data", h4("Data set"),
               choices = list("Training" = "training", "Validation" = "validation"),
               selected = "validation")
)

# Display plot output:
plotOutput("distPlot")
```

```{r var3_plot_shiny_server, context = "server", echo = FALSE}

# Render Plot output:
output$distPlot <- renderPlot({
  
  history %>% 
  filter(nlayers == input$layers, metric == input$metric, data == input$data) %>% 
  ggplot(aes(epoch, value, col = as_factor(as.character(powerto)))) +
  geom_line(alpha = 0.6) +
  geom_point(data = history[history$nlayers == 2 & 
                            history$powerto == 16 & 
                            history$metric == input$metric & 
                            history$data == input$data,], col = "red") +
  scale_color_brewer("Number of neurons", palette = "Blues") +
  labs(title = paste(input$layers, "hidden layers, changing number of neurons, -", input$data, "set.")) +
  theme_classic() +
  theme(panel.background = element_rect(fill = plot_bkg),
        legend.key = element_rect(fill = plot_bkg))
  
})
```

In some definitions, accuracy is increased by having more layers with fewer neurons or fewer layers with more neurons.

# Variant 4: mse loss function instead of binary crossentropy

What would happen if we used the the *mean squared error* (mse) instead of *binary crossentropy*? First, let's define a and train a new model, keeping everything as we have so far, except using `"mse"` as the loss function.

```{r var4_run, echo = FALSE, context = "data", cache = TRUE}
network <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

network %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("accuracy")
)

network %>% fit(
  train_data_vec,
  train_labels,
  epochs = 20,
  batch_size = 512,
  validation_data = list(val_data_vec, val_labels),
  verbose = FALSE) -> history_mse
```

Plot only mse variant:

```{r}
plot(history_mse)
```

Now let's compare it to our original model, using binary crossentropy:

```{r var4_plot, echo = FALSE, message = FALSE}
history_mse %>% 
  data.frame() %>% 
  mutate(nlayers = "2",
         powerto = "16",
         loss = "mse") -> history_mse

# merge with original
history_original %>% 
  mutate(loss = "binary crossentropy") %>% 
  full_join(history_mse) ->  history_mse

ggplot(history_mse, aes(epoch, value, col = loss, alpha = loss)) +
  geom_line(alpha = 0.6) +
  geom_point(col = "red") +
  scale_alpha_manual("Loss function", values = c(1,0)) +
  scale_color_brewer("Loss function", palette = "Set1") +
  facet_grid(metric ~ data, scales = "free_y") +
  labs(title = "Comparing binary crossentropy and mse loss functions") +
  theme_classic() +
  theme(panel.background = element_rect(fill = plot_bkg),
        legend.key = element_rect(fill = plot_bkg))

```

The accuracy is not affected, but the loss function is much lower. Moreover, it remains low, even after the binary crossentropy loss function begins to rise. Let's take a close look at the functions. The function for the mse is

$$\operatorname{MSE} = \frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n} = \frac{\sum_{i=1}^n{e_i^2}}{n}$$

We'll take a detailed look at the mse in the next case study, with the Boston housing price regression.

The actual function for the binary crossentropy is a simplified version of the categorical crossentropy we've used previously. It is

$$\operatorname{Binary Crossentropy} = \frac{-\sum_{i=1}^n[y_i \log p_i + (1-y_i) \log (1-p_i)]}{n}$$

Where $N$ is the number of instances, $y_i$ is the actual value and $p_i$ is the probability of assigning label to the instance $i$.

In R we can write the two functions as such:

```{r}
MSE <- function(actual, predicted) {
  sum(actual - predicted)^2/length(actual)
}

BinCross <- function(actual, predicted, eps = 1e-15) {
  predicted = pmin(pmax(predicted, eps), 1-eps)
  - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
}
# When calculating binary crossentropy (aka log loss) as per this formula, the predicted input values of 0 and 1 are actually undefined, so the function adjusts the predicted probabilities (p) by some very small amount, epsilon.
```

Thus, we can calculate the loss for our validation set as such: (this is not run, and our current network is using mse, so we have to redefine it.)

```{r calc_loss, eval = FALSE}
IMDB_predicted <- network %>% predict_classes(val_data_vec) %>% .[,1]
IMDB_actual <- val_labels

BinCross(IMDB_actual, IMDB_predicted)
MSE(IMDB_actual, IMDB_predicted)
```

The most intuitive way to understand what's happening is to use some play data. Let's begin with four instances.

```{r calc_loss_play, cache = T}
# Predicted
IMDB_predicted <- c(1,1,1,1)

# Actual
IMDB_actual <- list(c(0,0,0,0), # 4 wrong
                    c(1,0,0,0), # 3 wrong
                    c(1,1,0,0), # 2 wrong
                    c(1,1,1,0), # 1 wrong
                    c(1,1,1,1) # 0 wrong
                    )

data.frame(wrong = (4:0),
           binary = map_dbl(IMDB_actual, ~ BinCross(., IMDB_predicted)),
           mse = map_dbl(IMDB_actual, ~ MSE(., IMDB_predicted))) %>% 
  gather(key, value, -wrong) %>% 
  ggplot(aes(as.numeric(wrong), value, col = key)) +
  geom_line() +
  geom_point() +
  scale_color_brewer("loss function", palette = "Set1") +
  labs(x = "Number of incorrect predictions", 
       y = "loss value",
       title = "mse vs binary crossentropy loss values, n = 4 instances") +
  theme_classic() +
  theme(panel.background = element_rect(fill = plot_bkg),
        legend.key = element_rect(fill = plot_bkg))
```

This tells us that binary crossentropy carries a much larger penalty for a very confident incorrect prediction (0, where actual is 1) compared to mse. So although the loss is lower, the measure is not actually what we're looking for. Although it's mathematically possible, we would never use mse on a binary classification problem. We'll stick to binary crossentropy and a sigmod activation function in the last layer and use mse with a linear activation function in the last layer with regression in the next case study.

# Variant 5: tanh activation instead of relu

The tanh activation function was popular in the early days of neural networks. The function is:

After the training the model

```{r var5_run, context = "data", cache = TRUE}
network <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "tanh", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "tanh") %>% 
  layer_dense(units = 1, activation = "sigmoid")

network %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

network %>% fit(
  train_data_vec,
  train_labels,
  epochs = 20,
  batch_size = 512,
  validation_data = list(val_data_vec, val_labels),
  verbose = FALSE) -> history_tanh
```

Plot only tanh:

```{r}
plot(history_tanh)
```

Comparison to relu:

```{r var5_plot, echo = FALSE, message = FALSE}
history_tanh %>% 
  data.frame() %>% 
  mutate(nlayers = "2",
         powerto = "16",
         activation = "tanh") -> history_tanh

# merge with original
history_original %>% 
  mutate(activation = "relu") %>% 
  full_join(history_tanh) ->  history_tanh

ggplot(history_tanh, aes(epoch, value, col = activation, alpha = activation)) +
  geom_line(alpha = 0.6) +
  geom_point(col = "red") +
  scale_alpha_manual("Activation function", values = c(1,0)) +
  scale_color_brewer("Activation function", palette = "Set1") +
  facet_grid(metric ~ data, scales = "free_y") +
  labs(title = "Comparing relu and tanh activation functions") +
  theme_classic() +
  theme(panel.background = element_rect(fill = plot_bkg),
        legend.key = element_rect(fill = plot_bkg))
```

It appears that *tanh* results in a higher loss value, although the accuracy is not strongly affected.



