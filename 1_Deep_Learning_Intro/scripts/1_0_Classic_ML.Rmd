---
title: "The Boston House Price Dataset"
subtitle: "Classic Machine Learning Approaches"
author: "Rick Scavetta"
output:
  html_document:
  fig_caption: true
  toc: true
  toc_float:
    collapsed: false
    smooth_scroll: false
  toc_depth: 2
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, eval = TRUE)

# Initialize packages
library(keras)
library(tidyverse)
library(caret)
library(randomForest)
library(kableExtra)
```

# Boston regression Example:

For this example, I'm going to use the exact same data that we'll see later in deep learning so we can directly compare the results.

```{r}

# Read the dataset
boston <- read.csv("../data/boston_keras.csv")
```

We have 14 variables. The first 13 are the predictor variables and that last, `MEDV` is the response

| Variable | Description |
|:------|:---------------------------------------------------------|
| `CRIM` | per capita crime rate by town |
| `ZN` | proportion of residential land zoned for lots over 25,000 sq.ft |
| `INDUS` | proportion of non-retail business acres per town |
| `CHAS` | Charles River dummy variable (1 if tract bounds river; else 0) |
| `NOX` | nitric oxides concentration (parts per 10 million) |
| `RM` | average number of rooms per dwelling |
| `AGE` | proportion of owner-occupied units built prior to 1940 |
| `DIS` | weighted distances to five Boston employment centres |
| `RAD` | index of accessibility to radial highways |
| `TAX` | full-value property-tax rate per $10,000 |
| `PTRATIO` | pupil-teacher ratio by town |
| `B` | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town |
| `LSTAT` | % lower status of the population |
| `MEDV` | Median value of owner-occupied homes in $1000â€™s |

```{r}
summary(boston)
```

## Check features:

Correlation of each independent variable with the dependent variable

```{r}
cor(boston,boston$MEDV) %>% 
  data.frame() %>% 
  knitr::kable() %>% 
  kable_styling(full_width = F, position = "left")
```

Calulate near zero variance 

```{r}
nzv <- nearZeroVar(boston, saveMetrics = TRUE)
```

There are `r sum(nzv$nzv)` zero variance of near-zero variance variables.

# Create test set

```{r}
# as per the keras data set:
index <- 1:404

training <- boston[index,]
testing <- boston[-index,]
```

## Scaling:

We'll perform z-score transformation on each predictor variable, after splitting, as we will do later in deep learning.

```{r}
# Using dplyr to keep data frame structure:
training %>% 
  mutate_at(vars(-MEDV), scale) -> training

summary(training)
```


```{r}
# Using dplyr to keep data frame structure:
testing %>% 
  mutate_at(vars(-MEDV), scale) -> testing

summary(testing)
```

All variables have a mean of 0.

# GLM using all features


Our linear model will take the form:

$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_p X_{ip} + \epsilon_i$$. The coefficients are:

```{r}
fit_lm <- lm(MEDV~.,data = training)

data.frame(coef = round(fit_lm$coefficients,2)) %>% 
  knitr::kable()
```


```{r}
#predict on test set
pred_lm <- predict(fit_lm, newdata = testing)

MAE_lm <- sum(abs(pred_lm - testing$MEDV))/102
```

Our measure for the error will be the mean absolute error (MAE):

$$\mathrm{MAE} = \frac{\sum_{i=1}^n\left| y_i-\hat{y_i}\right|}{n} = \frac{\sum_{i=1}^n\left| e_i\right|}{n}$$

where $\hat{y_i}$ is the predicted value and $y_i$ is the actual value, the label. We'll see this again in deep learning and it's a more intuitive unit than the root-mean-square error (RMSE), which is also common. Recall that the RMSE is just the square of the the MSE:

$$\operatorname{MSE} = \frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n} = \frac{\sum_{i=1}^n{e_i^2}}{n}$$

$$\operatorname{RMSE} = \sqrt{MSE}$$

the MSE will also make a reappearance in deep learning as our loss function.

The MAE using the linear model is `r MAE_lm`. In dollar amounts, we are off by `r round(MAE_lm * 1000)`, much better than before.

## Random Forest

Let's give it another go using a differen method.

```{r}
fit_rf <- randomForest(MEDV ~ ., data = training)

pred_rf <- predict(fit_rf, testing)

MAE_rf <- sum(abs(pred_rf - testing$MEDV))/102

```

In this case the MAE is `r MAE_rf`. In dollar amounts, we are off by `r round(MAE_rf * 1000)`, much better than before.

## Visualizing output

```{r echo = FALSE}
data.frame(Actual = testing$MEDV,
           `GLM` = pred_lm,
           `Random Forest` = pred_rf) %>% 
  gather(Measure, Prediction, -Actual) %>% 
  ggplot(aes(Actual, Prediction)) +
  geom_point(shape = 16, alpha = 0.65) +
  geom_abline(slope = 1, intercept = 0, col = "dark red") +
  coord_fixed() +
  facet_grid(. ~ Measure) +
  theme_classic() +
  theme(axis.text = element_text(colour = "black"),
        strip.background = element_rect(colour = NA, fill = "gray92"))
```

```{r, eval = FALSE, echo = FALSE}
data.frame(Actual = testing$MEDV,
           `GLM` = pred_lm,
           `Random Forest` = pred_rf) %>% 
  rio::export("ClassicML.csv")
```

The results are saved in the `ClassicML.csv` file so that we can comper them with the results from deep learning.