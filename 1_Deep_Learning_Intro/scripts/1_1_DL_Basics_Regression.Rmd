---
title: "Introduction to Deep Learning"
subtitle: "Regression, no validation"
author: "Rick Scavetta"
output:
  html_document:
  fig_caption: true
  toc: true
  toc_float:
    collapsed: false
    smooth_scroll: false
  toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, eval = TRUE, cache.path = "cache/")

# Initialize packages
library(keras)
library(tidyverse)
```

# Session 1 {.tabset .tabset-fade .tabset-pills}

## Intro

### Learning Goals

Developing deep learning to two core questions in supervised learning: Classification and Regression. 

The UCI Abalone data-set is a small and easy starting point since it can be used for predicting age as either a categorical or continuous variable, leading to the 

### Outline

- What is a tensor and why use it?
- What is keras and what is its relationship to TensorFlow?
- What is the deep in deep learning? ANNs and densely-connected networks.
- The math of deep learning: Basics of matrix algebra, gradient descent, backpropagarion, chain rule.
- The four stages of Deep learning.
- Parameters and hyper-parameter.
- Functions distinguishing classification and regression: loss and optimizer functions.

### Functions in this session:

Basic `keras` functions:

| Function                   | Description                                       |
|:---------------------------|:--------------------------------------------------| 
| [`keras_model_sequential()`](https://www.rdocumentation.org/packages/keras/versions/2.2.0/topics/keras_model_sequential) | Keras network (model) composed of a linear stack of layers. |
| `layer_dense()`	           | Add a densely-connected NN layer to an output.    |
| `compile()`                | Configure a Keras model for training.             |
| `fit()`                    | Train a Keras model.                              |


In this case study, we'll perform a regression to predict a continouous response variable from 13 predictor variables. To accommodate for this different analytical problem, we'll use:

- A new normalization for the input data, z scores,
- A loss function, `mse`,
- A metric, `mae`, and
- No final activation function (i.e. scalar).

And since we have a really small data set we'll have:

- A very simple network architecture, and
- K-fold crossvalidation.

## Install tensorflow 

It's only necessary to run this once. 

```{r install, eval = F}
# for GPU
# install_keras(tensorflow = "gpu")

# or CPU:
# install_keras() # for cpu
```

# Part 1: Data Preparation

## Obtain data

```{r data, warning = FALSE}
c(c(train_data, train_targets), c(test_data, test_targets)) %<-% dataset_boston_housing()
```

## Examine data:

Our predictor variables:

```{r strDataPre}
str(train_data)
str(test_data)
```

The target, response variable:

```{r strTargets}
str(train_targets)
```

## Prepare the data:

Convert z-scores:

$$z_i=\frac{x_i-\bar{x}}{s}$$
```{r zScores, cache = T}
# parameters for Scaling:
mean <- colMeans(train_data) # mean of each column
std <- apply(train_data, 2, sd) # stdev of each column

# Calculate feature-wise (within-variable) z-scores: (x - mean)/std
train_data <- scale(train_data, center = mean, scale = std)
test_data <- scale(test_data, center = mean, scale = std)

```


```{r}
summary(train_data)
```


```{r}
summary(test_data)
```

The targes are the same:

```{r}
summary(train_targets)
```

```{r}
summary(test_targets)
```


# Part 2: Define Network

## Define the network as a function

In contrast to our previous case studies, we're going to call the same model multiple times. So we'll create a function with no arguments that we can call to create our model when ever we want to use it for training. 

Here, I've hardcoded the number of features for this dataset (`13`). To generalize, we could just use `dim(train_data)[2]` to get the number of dimensions from the training set.  

```{r defModel}
network <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = 13) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 1) 

network %>% compile(
  optimizer = "rmsprop", 
  loss = "mse", 
  metrics = "mae"
)
```

Note two key functions here, the mean squared error:

$$\operatorname{MSE} = \frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n} = \frac{\sum_{i=1}^n{e_i^2}}{n}$$
and the mean absolute error (MAE):

$$\mathrm{MAE} = \frac{\sum_{i=1}^n\left| y_i-\hat{y_i}\right|}{n} = \frac{\sum_{i=1}^n\left| e_i\right|}{n}$$
where $\hat{y_i}$ is the predicted value, given in our last single-unit layer, and $y_i$ is the actual value, the label.

We need to decid how many epochs to run our model plus the size and the number of the hidden layers.we'll train our model on the training data, then look at its performance on the test data:

```{r runZ_1}

# Train the model.
network %>% 
  fit(train_data, 
      train_targets,
      epochs = 120, 
      batch_size = 16, 
      verbose = TRUE)


```

`verbose = TRUE` prints all the values to the screen. That's fine, but we can also save the results to an object as it trains:

```{r}
history <- network %>% 
  fit(train_data, 
      train_targets,
      epochs = 120, 
      batch_size = 16, 
      verbose = TRUE)

```


Let's display its loss and accuracy curves:

```{r}
plot(history)
```


# Model evaluation:

```{r runZ_2}
result <- network %>% 
  evaluate(test_data, test_targets)

```

# Results:

```{r runZ_3}
MAE_dl <- result$mean_absolute_error
```


```{r resultsZ}
result
```

We are still off by about `r round(result$mean_absolute_error * 1000)`.

get the actual predictions:

```{r}

# summary(test_data)

read.csv("../data/ClassicML.csv") %>% 
  mutate(`Deep Learning` = predict(network, test_data)[,1]) %>% 
  gather(Measure, Prediction, -Actual) %>% 
  ggplot(aes(Actual, Prediction)) +
  geom_point(shape = 16, alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, col = "dark red") +
  coord_fixed(xlim = c(0,50), ylim = c(0,50), expand = 0, clip = "off") +
  facet_grid(. ~ Measure) +
  theme_classic() +
  theme(axis.text = element_text(colour = "black"),
        strip.background = element_rect(colour = NA, fill = "gray92"))

```
